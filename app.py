import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    # å¿…è¦ã«å¿œã˜ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    import os
    os.system('pip install -q transformers torch fugashi ipadic unidic-lite')
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ„Ÿæƒ…åˆ†æ",
    page_icon="ğŸ­",
    layout="wide"
)

# ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¤ãƒ³ãƒˆãƒ­
st.title("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ„Ÿæƒ…åˆ†æã‚¢ãƒ—ãƒª")
st.write("ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€AIãŒæ„Ÿæƒ…ã‚’åˆ†æã—ã¦ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã¾ã™ï¼")

# æ„Ÿæƒ…ã®ãƒªã‚¹ãƒˆ
emotion_names_jp = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼']
colors = [
    '#FF6384',  # èµ¤ (å–œã³)
    '#36A2EB',  # é’ (æ‚²ã—ã¿)
    '#FFCE56',  # é»„ (æœŸå¾…)
    '#4BC0C0',  # ç·‘ (é©šã)
    '#9966FF',  # ç´« (æ€’ã‚Š)
    '#FF9F40',  # ã‚ªãƒ¬ãƒ³ã‚¸ (æã‚Œ)
    '#C7C7C7',  # ã‚°ãƒ¬ãƒ¼ (å«Œæ‚ª)
    '#53E1A2'   # ãƒŸãƒ³ãƒˆ (ä¿¡é ¼)
]

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'model' not in st.session_state or 'tokenizer' not in st.session_state:
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰
    with st.spinner('ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™'):
        try:
            # å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆæ¥ç¶šå•é¡Œã‚’å›é¿ï¼‰
            import os
            os.environ['HF_HUB_OFFLINE'] = '0'
            
            # èªè¨¼ç„¡ã—ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            checkpoint = 'cl-tohoku/bert-base-japanese-char'  # æ–‡å­—ãƒ™ãƒ¼ã‚¹ã®è»½é‡ãƒ¢ãƒ‡ãƒ«
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ - è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            st.session_state.tokenizer = AutoTokenizer.from_pretrained(
                checkpoint, 
                use_fast=True,
                use_auth_token=False,
                trust_remote_code=True
            )
            
            st.session_state.model = AutoModelForSequenceClassification.from_pretrained(
                checkpoint, 
                num_labels=len(emotion_names_jp),
                use_auth_token=False,
                trust_remote_code=True
            )
            
            st.success('ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼')
            st.session_state.demo_mode = False
            
        except Exception as e:
            st.error(f'ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}')
            st.warning('ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãªæ„Ÿæƒ…å€¤ã‚’ä½¿ç”¨ï¼‰')
            
            # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨­å®š
            class DummyModel:
                def eval(self):
                    pass
                
                def __call__(self, **kwargs):
                    class DummyOutput:
                        def __init__(self):
                            self.logits = np.random.rand(1, len(emotion_names_jp))
                    return DummyOutput()
            
            class DummyTokenizer:
                def __call__(self, text, truncation=True, return_tensors="pt"):
                    return {}
            
            st.session_state.model = DummyModel()
            st.session_state.tokenizer = DummyTokenizer()
            st.session_state.demo_mode = True

# Softmaxé–¢æ•°
def np_softmax(x):
    f_x = np.exp(x) / np.sum(np.exp(x))
    return f_x

# æ„Ÿæƒ…åˆ†æé–¢æ•°
def analyze_emotion(text):
    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
    if st.session_state.get('demo_mode', True):
        # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªå€¤ã‚’ç”Ÿæˆ
        import random
        random_values = [random.random() for _ in range(len(emotion_names_jp))]
        # åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£è¦åŒ–
        total = sum(random_values)
        normalized = [v/total for v in random_values]
        return {n: float(p) for n, p in zip(emotion_names_jp, normalized)}
    
    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
    try:
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
        st.session_state.model.eval()

        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å¤‰æ› + æ¨è«–
        tokens = st.session_state.tokenizer(text, truncation=True, return_tensors="pt")
        preds = st.session_state.model(**tokens)
        prob = np_softmax(preds.logits.cpu().detach().numpy()[0])
        return {n: float(p) for n, p in zip(emotion_names_jp, prob)}
    except Exception as e:
        st.warning(f"æ„Ÿæƒ…åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.info("ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªå€¤ã‚’è¿”ã™
        import random
        random_values = [random.random() for _ in range(len(emotion_names_jp))]
        total = sum(random_values)
        normalized = [v/total for v in random_values]
        return {n: float(p) for n, p in zip(emotion_names_jp, normalized)}

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("### ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦")
st.write("ã“ã®ã‚¢ãƒ—ãƒªã¯æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ„Ÿæƒ…ã‚’åˆ†æã—ã€8ã¤ã®åŸºæœ¬æ„Ÿæƒ…ï¼ˆå–œã³ã€æ‚²ã—ã¿ã€æœŸå¾…ã€é©šãã€æ€’ã‚Šã€æã‚Œã€å«Œæ‚ªã€ä¿¡é ¼ï¼‰ã®ç¢ºç‡ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
st.write("åˆ†æã«ã¯æ±åŒ—å¤§å­¦ã®æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚")
